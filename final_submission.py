# Authors : Arvind Sreenivas, Dananjay Srinivas, Bhargav Shandilya

# -*- coding: utf-8 -*-
"""Final Submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U1k1jlHlfroSDBjnQhodUQzxsFNfRuac

"""


"""
Import statements start here

"""

import os
from tensorflow.keras.metrics import Recall, Precision, AUC
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk import word_tokenize
from nltk.corpus import stopwords
import nltk
from keras.layers import Dropout
from keras.callbacks import EarlyStopping
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from keras.models import Sequential
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
import seaborn as sns
import matplotlib.pyplot as plt
from urllib import request
import pandas as pd
import logging
import re
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
import numpy as np
nltk.download('stopwords')
nltk.download('punkt')
STOPWORDS = set(stopwords.words('english'))


"""# Fetch Don't Patronize Me! data manager module"""

module_url = f"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py"
module_name = module_url.split('/')[-1]
print(f'Fetching {module_url}')
# with open("file_1.txt") as f1, open("file_2.txt") as f2
with request.urlopen(module_url) as f, open(module_name, 'w') as outf:
    a = f.read()
    outf.write(a.decode('utf-8'))

""" 
Start of class definition for importing and splitting the dataset into test and train
Code borrowed from the official git repository for SemEval 2022: https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/

"""


class DontPatronizeMe:

    def __init__(self, train_path, test_path):

        self.train_path = train_path
        self.test_path = test_path
        self.train_task1_df = None
        self.train_task2_df = None
        self.test_set = None

    def load_task1(self):
        """
        Load task 1 training set and convert the tags into binary labels. 
        Paragraphs with original labels of 0 or 1 are considered to be negative examples of PCL and will have the label 0 = negative.
        Paragraphs with original labels of 2, 3 or 4 are considered to be positive examples of PCL and will have the label 1 = positive.
        It returns a pandas dataframe with paragraphs and labels.
        """
        rows = []
        with open(os.path.join(self.train_path, 'dontpatronizeme_pcl.tsv')) as f:
            for line in f.readlines()[4:]:
                par_id = line.strip().split('\t')[0]
                art_id = line.strip().split('\t')[1]
                keyword = line.strip().split('\t')[2]
                country = line.strip().split('\t')[3]
                t = line.strip().split('\t')[4].lower()
                l = line.strip().split('\t')[-1]
                if l == '0' or l == '1':
                    lbin = 0
                else:
                    lbin = 1
                rows.append(
                    {'par_id': par_id,
                     'art_id': art_id,
                     'keyword': keyword,
                     'country': country,
                        'text': t,
                        'label': lbin,
                     'orig_label': l
                     }
                )
        df = pd.DataFrame(rows, columns=[
                          'par_id', 'art_id', 'keyword', 'country', 'text', 'label', 'orig_label'])
        self.train_task1_df = df

    def load_test(self):
        #self.test_df = [line.strip() for line in open(self.test_path)]
        rows = []
        with open(self.test_path) as f:
            for line in f.readlines()[4:]:
                t = line.strip().split('\t')[3].lower()
                rows.append(t)
        self.test_set = rows


""" 

End of class definition for file import

"""

# helper function to save predictions to an output file


def labels2file(p, outf_path):
    with open(outf_path, 'w') as outf:
        for pi in p:
            outf.write(','.join([str(k) for k in pi])+'\n')


dpm = DontPatronizeMe('.', '.')

dpm.load_task1()



"""# Load paragraph IDs from file"""

trids = pd.read_csv('train_semeval_parids-labels.csv')
teids = pd.read_csv('dev_semeval_parids-labels.csv')

trids.head()

trids.par_id = trids.par_id.astype(str)
teids.par_id = teids.par_id.astype(str)


# Building the training set
rows = []  # will contain par_id, label and text
for idx in range(len(trids)):
    parid = trids.par_id[idx]

    text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id ==
                                  parid].text.values[0]
    label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id ==
                                   parid].label.values[0]
    rows.append({
        'par_id': parid,
        'text': text,
        'label': label
    })

trdf1 = pd.DataFrame(rows)


# Building the test set
rows = []  # will contain par_id, label and text
for idx in range(len(teids)):
    parid = teids.par_id[idx]
    text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id ==
                                  parid].text.values[0]
    label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id ==
                                   parid].label.values[0]
    rows.append({
        'par_id': parid,
        'text': text,
        'label': label
    })

len(rows)

# Build the test set
tedf1 = pd.DataFrame(rows)

# Downsample negative instances
pcldf = trdf1[trdf1.label == 1]
npos = len(pcldf)

# Create new training set post-downsampling
training_set1 = pd.concat([pcldf, trdf1[trdf1.label == 0][:npos*2]])
df = training_set1.reset_index(drop=True)

# Regex -> compile for bad symbols and removal of brackets and punctuation
remove_brackets = re.compile('[/(){}\[\]\|@,;]')
bad_symbols = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))


def clean_text(text):

    # text to lowercase
    text = text.lower()

    # removal of slashes and brackets
    text = remove_brackets.sub(' ', text)

    # Removal of unnecessary symbols
    text = bad_symbols.sub('', text)

    # Stopword removal
    text = ' '.join(word for word in text.split() if word not in STOPWORDS)
    return text


# Cleaning the training data
df['text'] = df['text'].apply(clean_text)
df['text'] = df['text'].str.replace('\d+', '')

# Cleaning test data
tedf1['text'] = tedf1['text'].apply(clean_text)
tedf1['text'] = tedf1['text'].str.replace('\d+', '')


MAX_NB_WORDS = 50000

MAX_SEQUENCE_LENGTH = 250

EMBEDDING_DIM = 100


# Tokenization of training set
tokenizer = Tokenizer(num_words=MAX_NB_WORDS,
                      filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(df['text'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

# Tokenization of test set
tokenizer2 = Tokenizer(num_words=MAX_NB_WORDS,
                       filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer2.fit_on_texts(tedf1['text'].values)
word_index = tokenizer2.word_index
print('Found %s unique tokens.' % len(word_index))

# One-hot encoding labels for train set
Y = pd.get_dummies(df['label']).values
print('Shape of label tensor:', Y.shape)

# Conversion of text to sequence -> input to model for training
X = tokenizer.texts_to_sequences(df['text'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)


# Test set tokenization
X_test = tokenizer.texts_to_sequences(tedf1['text'].values)
X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X_test.shape)

# One-hot encoding binary labels for test set
Y_test = pd.get_dummies(tedf1['label']).values
print('Shape of label tensor:', Y_test.shape)


print(X.shape[0])


""" 
Model definition starts here

"""

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.1))
model.add(LSTM(50, dropout=0.1, recurrent_dropout=0.1))
model.add(Dense(30, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(10, activation='relu'))
model.add(Dense(4))
model.add(Dense(2, activation='sigmoid'))
model.compile(loss='categorical_crossentropy', optimizer='adam',
              metrics=['accuracy', Precision(), Recall(), AUC()])
print(model.summary())

""" 
Model definition ends 

"""

# Training and hyperparameter tuning

epochs = 20
batch_size = 32

history = model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[
                    EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

preds = model.predict(X_test)

# Evaluation -> Test accuracy, precision, recall, and F1
preds_bin = []
for ele in preds:
    if ele[0] > ele[1]:
        preds_bin.append(0)
    else:
        preds_bin.append(1)

test_labels = tedf1['label'].tolist()

correct = []
for i, ele in enumerate(preds_bin):
    if ele == tedf1['label'].tolist()[i]:
        correct.append(1)

accuracy = accuracy_score(test_labels, preds_bin)
print('Accuracy: %f' % accuracy)

precision = precision_score(test_labels, preds_bin)
print('Precision: %f' % precision)

recall = recall_score(test_labels, preds_bin)
print('Recall: %f' % recall)

f1 = f1_score(test_labels, preds_bin)
print('F1 score: %f' % f1)


# Convert binary labels to .txt file

def labels2file(p, outf_path):
    with open(outf_path, 'w') as outf:
        for pi in p:
            outf.write(','.join([str(k) for k in pi])+'\n')


labels2file([[k] for k in preds_bin], 'task1.txt')
